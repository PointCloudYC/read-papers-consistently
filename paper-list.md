[TOC]

# Read papers consistently

a curated list of to-read papers on deep learning.

**Why doing things consistently is a a good habit?**

- simple answer is **accomplish anything you want.**
- For more, you can check the below inspiring articles. 
  - [5 Reasons Why Consistency is an Important Habit | Neel Raman](https://neelraman.com/5-reasons-why-consistency-is-an-important-habit/), 
  - [Why You MUST Do The Little Things Consistently - David Horsager](https://davidhorsager.com/why-you-must-do-the-little-things-consistently/),

## review papers

- Schmidhuber, “Deep Learning in Neural Networks” 
- Domingos, “A Few Useful Things to Know about Machine Learning” 
- Hinton, “[Top]Reducing the Dimensionality of Data with Neural Networks” 
- Halevy, Norvig, and Pereira, “[Top]The Unreasonable Effectiveness of Data”
- Bengio, Courville, and Vincent, “Representation Learning” 
- Jordan and Mitchell, “Machine Learning” 
- Hamilton, Ying, and Leskovec, “Representation Learning on Graphs” 

## classic foundational papers

- LeCun, Bengio, and Hinton, “1-Deep Learning”
- Krizhevsky, Sutskever, and Hinton, “2-ImageNet Classification with Deep Convolutional Neural Networks”
- He et al., “3-Deep Residual Learning for Image Recognition”
- Lecun et al., “4-Gradient-Based Learning Applied to Document Recognition”
- “5 Reasons Why Consistency Is an Important Habit”
- Szegedy et al., “5-Going Deeper with Convolutions”;
- Simonyan and Zisserman, “6-Very Deep Convolutional Networks for Large-Scale Image Recognition”
- Lin, Chen, and Yan, “7-Network In Network”
- Chollet, “Xception”
- Huang et al., “8-Densely Connected Convolutional Networks”
- Szegedy et al., “9-Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning”
- Goodfellow et al., “10-Generative Adversarial Networks”
- Kingma and Ba, “Adam”; 
- Ioffe and Szegedy, “Batch Normalization” 
- Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting”
- Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks.”
- [Xception](http://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html)
- [Network In Network](https://arxiv.org/abs/1312.4400), [network in network review](https://github.com/PointCloudYC/read-papers-consistently/nin.md)
- [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)
- [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Visualizing and Understanding Convolutional Networks | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)

## advanced papers

- Jaderberg et al., “Spatial Transformer Networks”
- Chen et al., “Drop an Octave”
- Xu, Evans, and Qi, “Feature Squeezing”Szegedy et al., “9-Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning”
- Hinton, Osindero, and Teh, “A Fast Learning Algorithm for Deep Belief Nets”
- Liu et al., “An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution”
- Marcus, “Deep Learning”
- Hinton, Vinyals, and Dean, “Distilling the Knowledge in a Neural Network”
- Selvaraju et al., “Grad-CAM”Vedaldi and Lenc, “MatConvNet”
- Szegedy et al., “Rethinking the Inception Architecture for Computer Vision”
- Zamir et al., “Taskonomy”
- Bengio, “[Top]Practical Recommendations for Gradient-Based Training of Deep Architectures”
- Glorot and Bengio, “[Top]Understanding the Difﬁculty of Training Deep Feedforward Neural Networks”
- Tremblay et al., “Training Deep Networks with Synthetic Data”
- Cheng et al., “Wide & Deep Learning for Recommender Systems.”


## Point cloud processing Using DL

### PC Segmentation and classfication

* [splatnet: SPLATNet: Sparse Lattice Networks for Point Cloud Processing (CVPR2018)](https://github.com/NVlabs/splatnet)
* [SO-Net: SO-Net: Self-Organizing Network for Point Cloud Analysis, CVPR2018](https://github.com/lijx10/SO-Net)
* [fully-convolutional-point-network: Fully-Convolutional Point Networks for Large-Scale Point Clouds](https://github.com/drethage/fully-convolutional-point-network)
* [superpoint_graph: Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs](https://github.com/loicland/superpoint_graph)
* [depth clustering](https://github.com/PRBonn/depth_clustering)
* [3d-semantic-segmentation: This work is based on our paper Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds, which is appeared at the IEEE International Conference on Computer Vision (ICCV) 2017, 3DRMS Workshop.](https://github.com/VisualComputingInstitute/3d-semantic-segmentation)

### PC Detection

* [Class-balanced-Grouping-and-Sampling-for-Point-Cloud-3D-Object-Detection: Winner's Code and Technical Report for the nuScenes 3D Object Detection challenge in WAD, CVPR 2019](https://github.com/poodarchu/Class-balanced-Grouping-and-Sampling-for-Point-Cloud-3D-Object-Detection)
* PointPillar
* [ApproxMVBB: Fast algorithms to compute an approximation of the minimal volume oriented bounding box of a point cloud in 3D.](https://github.com/gabyx/ApproxMVBB), approximation of the bounding box which is useful for the detection task.
* [3D_CNN_tensorflow: KITTI data processing and 3D CNN for Vehicle Detection](https://github.com/yukitsuji/3D_CNN_tensorflow)

### Other(registration,etc.)
* [learning_to_sample: A learned sampling approach for point clouds (CVPR 2019)](https://github.com/orendv/learning_to_sample), FPS vs the learning_to_sample
* [Super4PCS: Efficient Global Point-cloud registration](https://github.com/nmellado/Super4PCS)
